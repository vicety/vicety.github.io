

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://cdn.jsdelivr.net/gh/vicety/Images@master/images/momiji-ico.ico">
  <link rel="icon" href="https://cdn.jsdelivr.net/gh/vicety/Images@master/images/momiji-ico.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="vicety">
  <meta name="keywords" content="">
  
    <meta name="description" content="一步步将算子效率从 CPU 并行版本的 0.15 倍提升到 55 倍">
<meta property="og:type" content="article">
<meta property="og:title" content="使用 Nsight Compute 对算子进行优化">
<meta property="og:url" content="https://vicety.github.io/2023/12/10/Analysis-Driven-Optimization/index.html">
<meta property="og:site_name" content="vicety的博客">
<meta property="og:description" content="一步步将算子效率从 CPU 并行版本的 0.15 倍提升到 55 倍">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/vicety/Images/master/imagesADO-v2-roofline.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/vicety/Images/master/imagesADO-v3-roofline.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/vicety/Images/master/imagesADO-v3-MemWorkload-SMem.jpg">
<meta property="og:image" content="https://raw.githubusercontent.com/vicety/Images/master/imagesADO-v4-roofline.jpg">
<meta property="article:published_time" content="2023-12-10T09:55:00.000Z">
<meta property="article:modified_time" content="2024-02-18T17:50:44.851Z">
<meta property="article:author" content="vicety">
<meta property="article:tag" content="CUDA">
<meta property="article:tag" content="性能优化">
<meta property="article:tag" content="Profiling">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/vicety/Images/master/imagesADO-v2-roofline.jpg">
  
  
  
  <title>使用 Nsight Compute 对算子进行优化 - vicety的博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"vicety.github.io","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"left","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"TdxzU2OpwN7XHsa0T05Y01Kn-gzGzoHsz","app_key":"lfgF4JmupaIQkD5xRHAIBfTk","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 90vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>vicety的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://raw.githubusercontent.com/vicety/Images/master/images113949971_p0.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="使用 Nsight Compute 对算子进行优化"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-12-10 17:55" pubdate>
          2023年12月10日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          17k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          139 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="padding-left: 2rem; margin-right: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">使用 Nsight Compute 对算子进行优化</h1>
            
            
              <div class="markdown-body">
                
                <p>pid&#x3D;113949971</p>
<p>本文是 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/analysis-driven-optimization-preparing-for-analysis-with-nvidia-nsight-compute-part-1">Analysis-Driven Optimization: Finishing the Analysis with NVIDIA Nsight Compute</a> 系列文章（共三部分）的阅读笔记，对学习 Nsight Compute 这一 profiling 工具很有帮助，同时也推荐作者的另一篇 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-nsight-compute-to-inspect-your-kernels/">Using Nsight Compute to Inspect your Kernels</a> 作为入门</p>
<p>作者基于一个构造出的 Workload，使用 Nsight Compute 分析不同版本代码实现的性能瓶颈并指导优化。</p>
<h2 id="Workload"><a href="#Workload" class="headerlink" title="Workload"></a>Workload</h2><p>三维矩阵求平均 + 矩阵乘法</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs py">N = <span class="hljs-number">1024</span><br>L = <span class="hljs-number">1024</span><br>M = <span class="hljs-number">1024</span><br>inp = torch.randn(N, L, M)<br>mean = inp.mean(dim=<span class="hljs-number">2</span>) <span class="hljs-comment"># N, L</span><br>mat = torch.randn(L, L)<br>out = (mat @ mean)<br></code></pre></td></tr></table></figure>

<p>笔者机器 Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz (104核)<br>Tesla V100S-PCIE-32GB CUDA 12.2 驱动 535.54.03</p>
<h2 id="CPU-版本（并行-0-362923s-x2F-非并行-4-71439s）"><a href="#CPU-版本（并行-0-362923s-x2F-非并行-4-71439s）" class="headerlink" title="CPU 版本（并行 0.362923s &#x2F; 非并行 4.71439s）"></a>CPU 版本（并行 0.362923s &#x2F; 非并行 4.71439s）</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// CPU execution time: 0.362923s</span><br><span class="hljs-comment">// CPU execution time (with OMP_NUM_THREADS=1): 4.71439s </span><br><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">cpu_version1</span><span class="hljs-params">(T *input, T *output, T *matrix, <span class="hljs-type">int</span> L, <span class="hljs-type">int</span> M, <span class="hljs-type">int</span> N)</span></span>&#123;<br><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> omp parallel for</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> k = <span class="hljs-number">0</span>; k &lt; N; k++)&#123;      <span class="hljs-comment">// repeat the following, N times</span><br>    <span class="hljs-function">std::vector&lt;T&gt; <span class="hljs-title">v1</span><span class="hljs-params">(L)</span></span>;           <span class="hljs-comment">// vector length of L</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; M; i++)     <span class="hljs-comment">// compute average vector over M input vectors</span><br>      <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; L; j++)<br>        v1[j] += input[k*M*L+j*M+i];<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; L; j++)<br>      v1[j] /= M;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; L; i++)     <span class="hljs-comment">// matrix-vector multiply</span><br>      <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; L; j++)<br>	output[i*N+k] += matrix[i*L+j]*v1[j];<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="GPU-版本一（2-37139s）"><a href="#GPU-版本一（2-37139s）" class="headerlink" title="GPU 版本一（2.37139s）"></a>GPU 版本一（2.37139s）</h2><p>grid &#x3D; 1<br>block &#x3D; 1024</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> inp.dim0<br>    inp.dim1 对应 block 做并行，每个 thread 对 inp.dim2 求平均<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> mat.dim0:<br>        smem[threadIdx.x] = mat[i][j] * mean[k][j]<br>        折半规约求和<br>        回写结果<br></code></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// gpu_version1&lt;&lt;&lt;1, L&gt;&gt;&gt;(d_input, d_output, d_matrix, L, M, N);</span><br><br><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">gpu_version1</span><span class="hljs-params">(<span class="hljs-type">const</span> T * __restrict__ input, T * __restrict__ output, <span class="hljs-type">const</span> T * __restrict__ matrix, <span class="hljs-type">const</span> <span class="hljs-type">int</span> L, <span class="hljs-type">const</span> <span class="hljs-type">int</span> M, <span class="hljs-type">const</span> <span class="hljs-type">int</span> N)</span></span>&#123;<br>  __shared__ T smem[my_L];<br>  <span class="hljs-type">size_t</span> idx = ((<span class="hljs-type">size_t</span>)blockIdx.x)*blockDim.x + threadIdx.x;<br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> k = <span class="hljs-number">0</span>; k &lt; N; k++)&#123;  <span class="hljs-comment">// iterate over N data sets</span><br>    T v1 = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; M; i++) <span class="hljs-comment">// perform vector averaging</span><br>      v1 += input[k*M*L+idx*M+i]; <span class="hljs-comment">// threadIdx 到 dim1</span><br>    v1 /= M;<br>    <span class="hljs-comment">// idx 覆盖一个 batch（N 维度）的整个向量</span><br>    <span class="hljs-comment">// loop on 矩阵第 i 行</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; L; i++)&#123; <span class="hljs-comment">// perform matrix-vector multiply</span><br>      __syncthreads();<br>      smem[threadIdx.x] = v1 * matrix[i*L+idx];<br>      <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> s = blockDim.x&gt;&gt;<span class="hljs-number">1</span>; s &gt; <span class="hljs-number">0</span>; s&gt;&gt;=<span class="hljs-number">1</span>)&#123;<br>        __syncthreads();<br>	      <span class="hljs-keyword">if</span> (threadIdx.x &lt; s) smem[threadIdx.x] += smem[threadIdx.x+s];<br>      &#125; <span class="hljs-comment">// 并行 reduction</span><br>      <span class="hljs-keyword">if</span> (!threadIdx.x) output[k+i*N] = smem[<span class="hljs-number">0</span>];<br>    &#125;<br>  &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>问题：gridDim&#x3D;1，SM 利用率极低，也可以直接在 Nsight Compute 的 SM 利用率中看到</p>
<h2 id="GPU-版本二（2-37139s-gt-0-083069s）"><a href="#GPU-版本二（2-37139s-gt-0-083069s）" class="headerlink" title="GPU 版本二（2.37139s -&gt; 0.083069s）"></a>GPU 版本二（2.37139s -&gt; 0.083069s）</h2><p>很容易想到 gridDim 可以对应 inp.dim0 维度的并行，提速 30 倍</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// gpu_version2&lt;&lt;&lt;N, L&gt;&gt;&gt;(d_input, d_output, d_matrix, L, M, N);</span><br><br><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">gpu_version2</span><span class="hljs-params">(<span class="hljs-type">const</span> T * __restrict__ input, T * __restrict__ output, <span class="hljs-type">const</span> T * __restrict__ matrix, <span class="hljs-type">const</span> <span class="hljs-type">int</span> L, <span class="hljs-type">const</span> <span class="hljs-type">int</span> M, <span class="hljs-type">const</span> <span class="hljs-type">int</span> N)</span></span>&#123;<br>  <span class="hljs-comment">// parallelize threadIdx.x over vector length, and blockIdx.x across k (N)</span><br>  __shared__ T smem[my_L];<br>  <span class="hljs-type">int</span> idx = threadIdx.x;<br>  <span class="hljs-type">int</span> k = blockIdx.x;<br>    T v1 = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; M; i++)  <span class="hljs-comment">// perform vector averaging</span><br>      v1 += input[k*M*L+idx*M+i];<br>    v1 /= M;<br>    <span class="hljs-comment">// 矩阵第 i 行</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; L; i++)&#123; <span class="hljs-comment">// perform matrix-vector multiply</span><br>      __syncthreads();<br>      smem[threadIdx.x] = v1 * matrix[i*L+idx];<br>      <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> s = blockDim.x&gt;&gt;<span class="hljs-number">1</span>; s &gt; <span class="hljs-number">0</span>; s&gt;&gt;=<span class="hljs-number">1</span>)&#123;<br>        __syncthreads();<br>        <span class="hljs-keyword">if</span> (threadIdx.x &lt; s) smem[threadIdx.x] += smem[threadIdx.x+s];&#125;<br>      <span class="hljs-keyword">if</span> (!threadIdx.x) output[k+i*N] = smem[<span class="hljs-number">0</span>];&#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="GPU-版本三（0-083069s-gt-0-021241s）"><a href="#GPU-版本三（0-083069s-gt-0-021241s）" class="headerlink" title="GPU 版本三（0.083069s -&gt; 0.021241s）"></a>GPU 版本三（0.083069s -&gt; 0.021241s）</h2><p>优化访存，提速四倍<br>（接下来开始大量使用 Nsight Compute）</p>
<h3 id="GPU-Speed-of-Light"><a href="#GPU-Speed-of-Light" class="headerlink" title="GPU Speed of Light"></a>GPU Speed of Light</h3><p>本栏指出</p>
<blockquote>
<p>[Warning] This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and&#x2F;or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.</p>
</blockquote>
<p>Scheduler Statistics 一栏的 <code>Issue Slot Utilization</code> Rule 指出</p>
<blockquote>
<p>[Warning] Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 16.00 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps either increase the number of active warps or reduce the time the active warps are stalled.</p>
</blockquote>
<p>scheduler 平均每 6.8 个 cycle 才能 issue 一个指令，参考 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions">CUDA C Programming Guide Arithmetic Instructions</a> 一节，7.x SM 每 cycle 可以执行 64 个 float32 加&#x2F;乘指令，每个 scheduler 对应 1&#x2F;4 个 SM，最大 16 个 warp，意味着每个 CUDA Core 每个 cycle 可以完成一个 float32 add&#x2F;mul&#x2F;add-mul，这和 16 个 warp 每 6.8 个 cycle 执行一个指令相去甚远。上面还指出，甚至有 31% 的 cycle 没有任何 warp 处于可以接受指令的状态</p>
<h3 id="Warp-State-Statistics"><a href="#Warp-State-Statistics" class="headerlink" title="Warp State Statistics"></a>Warp State Statistics</h3><p>本栏的 <code>Stall LG Throttle</code> Rule 指出</p>
<blockquote>
<p>On average, each warp of this kernel spends 89.6 cycles being stalled waiting for the L1 instruction queue for local and global (LG) memory operations to be not full. Typically, this stall occurs only when executing local or global memory instructions extremely frequently. Avoid redundant global memory accesses. Try to avoid using thread-local memory by checking if dynamically indexed arrays are declared in local scope, of if the kernel has excessive register pressure causing by spills. If applicable, consider combining multiple lower-width memory operations into fewer wider memory operations and try interleaving memory operations and math instructions. This stall type represents about 76.0% of the total average of 117.9 cycles between issuing two instructions.</p>
</blockquote>
<p>每个平均 warp 都有 89.6 个 cycle 在等待 L1 instruction queue 有空间，可能的触发条件：global 访存过多、使用了 thread local 内存（应当避免）、register 太多导致 spill</p>
<p>Question: 什么是 L1 instruction queue</p>
<blockquote>
<p>GPT:</p>
<ul>
<li>Local&#x2F;Global Instruction Queue: When warps execute memory instructions (reading from or writing to memory), these instructions are placed in a queue before they can be processed. NVIDIA GPU architectures have separate instruction queues for different kinds of memory operations. In this case, “local&#x2F;global” refers to the queue for memory operations that access either local memory (thread-specific memory) or global memory (accessible by all threads).</li>
<li>Not Full: For an instruction to be placed in the queue, there must be space available. If the queue is full, new instructions must wait until there is room, leading to stalls.</li>
</ul>
</blockquote>
<p>Question: MIO throttle、LG throttle、Tex throttle 分别是什么</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/66569433">https://stackoverflow.com/a/66569433</a><br>The MIO is a partition in the NVIDIA SM (starting in Maxwell) that contains execution units shared between the 4 warp schedulers or slower math execution units (e.g. XU pipe).<br>Instructions issued to these execution units are first issued into instruction queues allowing the warp schedulers to continue to issue independent instructions from the warp. <strong>If a warp’s next instruction is to an instruction queue that is full then the warp is stalled until the queue is not full and the instruction can be enqueued. When this stall occurs the warp will report a throttle reason based upon the instruction queue type.</strong> The mapping of instruction queues to pipes differs between chips. This is the general mapping.</p>
<ul>
<li>mio_throttle (ADU, CBU, LSU, XU)</li>
<li>lg_throttle (LSU)<ul>
<li>lg_throttle is used if MIO instruction queue reaches a watermark for local&#x2F;global instructions. Throttling local&#x2F;global instructions early allows SM to continue to issue shared memory instructions when L1 backpressure due to local&#x2F;global L1 misses.</li>
</ul>
</li>
<li>tex_throttle (TEX, FP64 on non-*100 chips, Tensor on TU11x)</li>
</ul>
<p>If the warp’s next instruction to issue is to a sub-partition specific execution unit (FMA, ALU, Tensor, FP64 (*100 GPUs) then the stall reason is math_throttle.</p>
</blockquote>
<p>Question: Roofline chart 指出虽然 Performance (FLOP&#x2F;s) 上升，但相比于版本二的 arithmetic intensity 下降了，为什么<br><img src="https://raw.githubusercontent.com/vicety/Images/master/imagesADO-v2-roofline.jpg" srcset="/img/loading.gif" lazyload><br>（我的理解）SM 利用率大幅提高提供了更高的 Performance，但也导致 SM 的 L1 instruction queue 打满，使得算子更加偏向 mem bound，对应 arithmetic intensity 下降。原先的算子没有到达 roofline 的 1%，新的实现虽然降低 itensity 导致上限降低，但是达到了 40% 的理论极限</p>
<h3 id="Source-Counters"><a href="#Source-Counters" class="headerlink" title="Source Counters"></a>Source Counters</h3><p>Scheduler Statistics 一栏推荐我们看 Warp State Analysis 与 Source Counters，Source Counters 指出</p>
<blockquote>
<p>This kernel has uncoalesced global accesses resulting in a total of 939524096 excessive sectors (78% of the total 1209008128 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide has additional information on reducing uncoalesced device memory accesses.</p>
</blockquote>
<p>Question: 什么是 sector</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html</a><br>Aligned 32 byte-chunk of memory in a cache line or device memory. An L1 or L2 cache line is four sectors, i.e. 128 bytes. Sector accesses are classified as hits if the tag is present and the sector-data is present within the cache line. Tag-misses and tag-hit-data-misses are all classified as misses.</p>
</blockquote>
<p>就是 32 byte 的一次访问</p>
<p>Question：什么是 wavefront</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html">https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html</a><br>Unique “work package” generated at the end of the processing stage for requests. All work items of a wavefront are processed in parallel, while work items of different wavefronts are serialized and processed on different cycles. At least one wavefront is generated for each request.</p>
<p>A wavefront is the maximum unit that can pass through that pipeline stage per cycle. If not all cache lines or sectors can be accessed in a single wavefront, multiple wavefronts are created and sent for processing one by one, i.e. in a serialized manner. Limitations of the work within a wavefront may include the need for a consistent memory space, a maximum number of cache lines that can be accessed, as well as various other reasons. Each wavefront then flows through the L1TEX pipeline and fetches the sectors handled in that wavefront. The given relationships of the three key values in this model are requests:sectors is 1:N, wavefronts:sectors 1:N, and requests:wavefronts is 1:N.</p>
<p>A wavefront is described as a (work) package that can be processed at once, i.e. there is a notion of processing one wavefront per cycle in L1TEX. Wavefronts therefore represent the number of cycles required to process the requests, while the number of sectors per request is a property of the access pattern of the memory instruction for all participating threads. For example, it is possible to have a memory instruction that requires 4 sectors per request in 1 wavefront. However, you can also have a memory instruction having 4 sectors per request, but requiring 2 or more wavefronts.</p>
</blockquote>
<p>（我的理解）一个 cycle 能处理完的数据量</p>
<h3 id="Memory-Workload-Analysis"><a href="#Memory-Workload-Analysis" class="headerlink" title="Memory Workload Analysis"></a>Memory Workload Analysis</h3><p>本栏指出从 Device 到 L2 的吞吐量达到理论上限的 40%<br>更重要的是，观察 L1&#x2F;TEX 相关指标，Global Load 共发送 67M request &#x2F; instruction，结果却引起了 1.2B 的 sector 读取（注意 request 是对于一整个 warp 来说的，因此 sector 可以大于 request）（这里的 sector 和 memory transaction 我的理解是一个意思），Sectors&#x2F;Req 达到 17.77（最坏情况可以达到 warp size，32）</p>
<p>Question: request 与 sector 的关系？</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/wahts-the-difference-between-wavefronts-and-sectors-req/165293/3">What’s the difference between ‘wavefronts’ and ‘sectors&#x2F;Req’?</a><br>When an SM executes a global&#x2F;local&#x2F;shared memory instruction for a warp, a single request is sent to L1TEX. This request communicates the information for all participating threads of this warp (up to 32 threads). For local and global memory, based on the access pattern and the participating threads, the request requires to access a number of cache lines, and sectors within these cache lines.</p>
</blockquote>
<blockquote>
<p>CUDA C Programming Guide 5.3.2<br>Global memory resides in device memory and device memory is accessed via 32-, 64-, or 128-byte memory transactions. These memory transactions must be naturally aligned: Only the 32-, 64-, or 128-byte segments of device memory that are aligned to their size (i.e., whose first address is a multiple of their size) can be read or written by memory transactions.</p>
<p>When a warp executes an instruction that accesses global memory, it coalesces the memory accesses of the threads within the warp into one or more of these memory transactions depending on the size of the word accessed by each thread and the distribution of the memory addresses across the threads. In general, the more transactions are necessary, the more unused words are transferred in addition to the words accessed by the threads, reducing the instruction throughput accordingly. For example, if a 32-byte memory transaction is generated for each thread’s 4-byte access, throughput is divided by 8.</p>
</blockquote>
<p>我们可以在代码中进一步确认这一访存问题的来源</p>
<h3 id="Page-Source"><a href="#Page-Source" class="headerlink" title="Page Source"></a>Page Source</h3><p>在右侧统计栏点击右键打开所有指标，可以观察到大部分的 warp stall 都来自于 <code>v1 += input[k*M*L+idx*M+i];</code> 这行，同时从 L2 Theoretical Sectors Global Excessive 以及 L2 Theoretical Sectors Global 可以看出，有 90% 左右的 sector 内容是未被利用的，这引导我们优化此行的访存方式</p>
<h3 id="版本三代码"><a href="#版本三代码" class="headerlink" title="版本三代码"></a>版本三代码</h3><p>这里给出的改进是将 block 的一部分放到矩阵行维度来并行，具体来说 thread 0-31 处理矩阵某行的第 32i + idx(0-31) 列，这样 32 个 thread 需要 32 个 float，可以填满一个 32 byte 的 memory txn。而余下的 block size 保留在矩阵行维度的并行，即一个 2D block (32, 32)</p>
<p>可以看到，解决访存瓶颈后，我们在 arithmetic intensity 上有所前进，也因此得到了更高的天花板，当前 kernel 达到了显卡 fp32 上 1% 的 peak performance<br><img src="https://raw.githubusercontent.com/vicety/Images/master/imagesADO-v3-roofline.jpg" srcset="/img/loading.gif" lazyload></p>
<p>查看 Sector&#x2F;Req，这次做到了 4.00，完美</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">gpu_version3</span><span class="hljs-params">(<span class="hljs-type">const</span> T * __restrict__ input, T * __restrict__ output, <span class="hljs-type">const</span> T * __restrict__ matrix, <span class="hljs-type">const</span> <span class="hljs-type">int</span> L, <span class="hljs-type">const</span> <span class="hljs-type">int</span> M, <span class="hljs-type">const</span> <span class="hljs-type">int</span> N)</span></span>&#123;<br>  <span class="hljs-comment">// parallelize threadIdx.x over vector length, and blockIdx.x across k (N)</span><br>  <span class="hljs-comment">// do initial vector reduction via warp-stride loop</span><br>  __shared__ T smem[my_L];<br>  <span class="hljs-type">int</span> idx = threadIdx.x;<br>  <span class="hljs-type">int</span> idy = threadIdx.y;<br>  <span class="hljs-type">int</span> id  = idy*warpSize+idx;<br>  <span class="hljs-type">int</span> k = blockIdx.x;<br>  T v1;<br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> y = threadIdx.y; y &lt; L; y+=blockDim.y)&#123; <span class="hljs-comment">// vertical block-stride loop</span><br>    v1 = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> x = threadIdx.x; x &lt; M; x+=warpSize)  <span class="hljs-comment">// horizontal warp-stride loop</span><br>      v1 += input[k*M*L+y*M+x];<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> offset = warpSize&gt;&gt;<span class="hljs-number">1</span>; offset &gt; <span class="hljs-number">0</span>; offset &gt;&gt;= <span class="hljs-number">1</span>) <span class="hljs-comment">// warp-shuffle reduction</span><br>       v1 += __shfl_down_sync(<span class="hljs-number">0xFFFFFFFF</span>, v1, offset);<br>    <span class="hljs-keyword">if</span> (!threadIdx.x) smem[y] = v1/M;&#125;<br>  __syncthreads();<br>  v1 = smem[id];<br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; L; i++)&#123;                     <span class="hljs-comment">// matrix-vector multiply</span><br>    __syncthreads();<br>    smem[id] = v1 * matrix[i*L+id];<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> s = (blockDim.x*blockDim.y)&gt;&gt;<span class="hljs-number">1</span>; s &gt; <span class="hljs-number">0</span>; s&gt;&gt;=<span class="hljs-number">1</span>)&#123;<br>      __syncthreads();<br>      <span class="hljs-keyword">if</span> (id &lt; s) smem[id] += smem[id+s];&#125;<br>    <span class="hljs-keyword">if</span> (!id) output[k+i*N] = smem[<span class="hljs-number">0</span>];&#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="GPU-版本四（0-021241s-gt-0-011824s）"><a href="#GPU-版本四（0-021241s-gt-0-011824s）" class="headerlink" title="GPU 版本四（0.021241s -&gt; 0.011824s）"></a>GPU 版本四（0.021241s -&gt; 0.011824s）</h2><p>继续优化访存，提速一倍</p>
<p>之前版本中 warp 中的 thread 每次访问矩阵不同行的相同列，这些列空间并不连续，导致 memory transaction 拿到的数据利用率很低</p>
<p>Question：虽然第一次访问效率很低，但这些数据就已经在 cache 了，那下一次访问效率就高了吧？<br>（和下面这个本质是相同的问题）<br>Question: 我正在用 CUDA 处理一个矩阵，每次读入一个 2D block &#x3D; n<em>m，对应矩阵的一个 n</em>m 的子矩阵，每次向右移动 m 个单位，走完一行后向下移动 n 个单位，这样可以使得每个 memory transaction 读入的数据都是完全被需要的，但如果我换一种访问方式，比如说，3<em>9 的矩阵，2D block 大小为 3</em>3，分三次访问完这个矩阵，0 号线程三次分别访问 0,0 0,3 0,6 三个位置，1 号线程三次分别访问 0,1 0,4 0,7 三个位置，这样的话，虽然第一次读入的 memory 中不是所有的数据都被这一次访问需要，但它们可能被下一次访问需要，请问这样能获得与我最开始介绍的访问方式相似的效率吗</p>
<blockquote>
<p>GPT: 在 CUDA 中处理矩阵时考虑 memory efficiency 是非常重要的，因为内存访问模式对整体性能有很大的影响。在 GPU 中，内存是通过更大的块（称为 memory transactions）读取的，而并非单个元素。这些块通常是 32 或 128 字节。当一个 warp（32 个线程）请求数据时，最理想的情况是所有线程读取的数据位于同一个或者几个连续的 memory transactions 中。这种情况称为 memory coalescing，它能够最大化 memory bandwidth 的利用率。<br>Question1: 我总结一下你的回答，看我的理解是否正确。第二种访问方式中，我的想法是虽然某些访问无法利用 memory coalescing，但后续的访问需要的内容可能已经在 L1 cache 了，我因此认为这有可能实现与第一种访问方式类似的性能。你的解释是我想要的 L1 cache 中的数据在后续访问时可能已经失效了，对吗？<br>GPT: 是的，你的理解基本上是正确的。在第二种访问方式中，你提到的某些访问可能无法利用 memory coalescing，但你期望后续访问时，所需的数据已经由于之前的访问而存在于 L1 cache 中。这确实是一个可能的情况，而且理论上可以减少对全局内存的访问次数，提高性能。<br>然而，我提到的是，在一些情况下，L1 cache 中的数据可能因为缓存替换策略被逐出，特别是<strong>在高度并行的环境下，多个 warp 会同时进行内存访问，可能导致缓存行频繁被替换。如果每次访问的数据集合足够大，以至于不能完全被缓存，或者内存访问模式导致缓存行被频繁替换，那么预期中的 cache hit 可能会降低，从而影响性能。</strong><br>Question2: 我想到的另一种可能是某个 SM 不一定总是执行同一个 warp 直到它处理完所有数据，有可能交替执行 block 内的多个 block，从而导致 L1 cache 失效，这种可能性也会导致第二种访问方式低效，对吗？<br>GPT: 由于 SM 上有限的缓存资源，如 L1 cache 和 shared memory，同时存在的多个 block 和 warp 可能会争夺这些资源。当 SM 在不同的 warps 间频繁进行上下文切换时，一个 warp 中的线程使用的缓存数据可能会被另一个 warp 的线程所替换。如果一个 warp 的执行被延迟，其缓存中的数据就有可能在该 warp 重新被调度之前被逐出，特别是当正在处理的数据集合超过了缓存容量时。</p>
</blockquote>
<h3 id="GPU-SOL"><a href="#GPU-SOL" class="headerlink" title="GPU SOL"></a>GPU SOL</h3><blockquote>
<p>[Warning] Compute is more heavily utilized than Memory: Look at the Compute Workload Analysis report section to see what the compute pipelines are spending their time doing. Also, consider whether any computation is redundant and could be reduced or moved to look-up tables.</p>
</blockquote>
<h3 id="Compute-Workload-Analysis"><a href="#Compute-Workload-Analysis" class="headerlink" title="Compute Workload Analysis"></a>Compute Workload Analysis</h3><p>没有 warning，但是 LSU 利用率最高，看起来仍然在访存上很重</p>
<p>TODO 本栏左右的区别 ALU FMA 与 LSU ALU</p>
<h3 id="Memory-Workload-Analysis-1"><a href="#Memory-Workload-Analysis-1" class="headerlink" title="Memory Workload Analysis"></a>Memory Workload Analysis</h3><p>看图发现 Shared Mem 达到了 55% 的最大带宽，意味着 LSU 访存量大多半是在访问这里</p>
<p><img src="https://raw.githubusercontent.com/vicety/Images/master/imagesADO-v3-MemWorkload-SMem.jpg" srcset="/img/loading.gif" lazyload></p>
<p>更进一步，Shared Memory 的压力主要来源于 Other，原文指出</p>
<blockquote>
<p>As a result of the sweep-style reduction construction considered across the threadblock, there are many iterations of the sweep reduction loop that have one or more entire warps that do not participate. They are predicated off, completely across the warp. These non-participating warps still contribute to shared-memory pressure, and this is reflected in the Other category in the screenshot.<br>TODO Why “non-participating warps still contribute to shared-memory pressure”?</p>
</blockquote>
<h3 id="Page-Source-1"><a href="#Page-Source-1" class="headerlink" title="Page Source"></a>Page Source</h3><p>继续看代码，发现 <code>if (id &lt; s) smem[id] += smem[id+s];&#125;</code> 一行是新的 wrap stall 原因，原文给出了一个两阶段的 reduction 算法。当前版本是 1024 维向量相乘结果做 10 次折半规约，新的方法是每 32 个线程做 5 次规约得到 32 个结果，最后再用 32 个线程规约一次得到一个最终结果，节约了 4 次规约</p>
<h3 id="版本四代码"><a href="#版本四代码" class="headerlink" title="版本四代码"></a>版本四代码</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">gpu_version4</span><span class="hljs-params">(<span class="hljs-type">const</span> T * __restrict__ input, T * __restrict__ output, <span class="hljs-type">const</span> T * __restrict__ matrix, <span class="hljs-type">const</span> <span class="hljs-type">int</span> L, <span class="hljs-type">const</span> <span class="hljs-type">int</span> M, <span class="hljs-type">const</span> <span class="hljs-type">int</span> N)</span></span>&#123;<br>  <span class="hljs-comment">// parallelize threadIdx.x over vector length, and blockIdx.x across k (N)</span><br>  <span class="hljs-comment">// do initial vector reduction via warp-stride loop</span><br>  __shared__ T smem[my_L];<br>  <span class="hljs-type">int</span> idx = threadIdx.x;<br>  <span class="hljs-type">int</span> idy = threadIdx.y;<br>  <span class="hljs-type">int</span> id  = idy*warpSize+idx;<br>  <span class="hljs-type">int</span> k = blockIdx.x;<br>  T v1;<br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> y = threadIdx.y; y &lt; L; y+=blockDim.y)&#123; <span class="hljs-comment">// vertical block-stride loop</span><br>    v1 = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> x = threadIdx.x; x &lt; M; x+=warpSize)  <span class="hljs-comment">// horizontal warp-stride loop</span><br>      v1 += input[k*M*L+y*M+x];<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> offset = warpSize&gt;&gt;<span class="hljs-number">1</span>; offset &gt; <span class="hljs-number">0</span>; offset &gt;&gt;= <span class="hljs-number">1</span>) <span class="hljs-comment">// warp-shuffle reduction</span><br>       v1 += __shfl_down_sync(<span class="hljs-number">0xFFFFFFFF</span>, v1, offset);<br>    <span class="hljs-keyword">if</span> (!threadIdx.x) smem[y] = v1/M;&#125;<br>  __syncthreads();<br>  v1 = smem[id];<br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; L; i++)&#123;                     <span class="hljs-comment">// matrix-vector multiply</span><br>    T v2 = v1 * matrix[i*L+id];<br>    <span class="hljs-comment">// 1st warp-shuffle reduction</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> offset = warpSize&gt;&gt;<span class="hljs-number">1</span>; offset &gt; <span class="hljs-number">0</span>; offset &gt;&gt;= <span class="hljs-number">1</span>)<br>       v2 += __shfl_down_sync(<span class="hljs-number">0xFFFFFFFF</span>, v2, offset);<br>    <span class="hljs-keyword">if</span> (idx == <span class="hljs-number">0</span>) smem[idy] = v2;<br>   __syncthreads(); <span class="hljs-comment">// put warp results in shared mem</span><br>    <span class="hljs-comment">// hereafter, just warp 0</span><br>    <span class="hljs-keyword">if</span> (idy == <span class="hljs-number">0</span>)&#123;<br>       <span class="hljs-comment">// reload v2 from shared mem if warp existed</span><br>       v2 = (idx &lt; ((blockDim.x*blockDim.y)&gt;&gt;<span class="hljs-number">5</span>))?smem[idx]:<span class="hljs-number">0</span>;<br>       <span class="hljs-comment">// final warp-shuffle reduction</span><br>       <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> offset = warpSize&gt;&gt;<span class="hljs-number">1</span>; offset &gt; <span class="hljs-number">0</span>; offset &gt;&gt;= <span class="hljs-number">1</span>)<br>          v2 += __shfl_down_sync(<span class="hljs-number">0xFFFFFFFF</span>, v2, offset);&#125;<br>    <span class="hljs-keyword">if</span> (!id) output[k+i*N] = v2;&#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>达到 5% 理论 fp32 性能峰值</p>
<p><img src="https://raw.githubusercontent.com/vicety/Images/master/imagesADO-v4-roofline.jpg" srcset="/img/loading.gif" lazyload></p>
<h3 id="最终版本（0-011824s-gt-0-006524s"><a href="#最终版本（0-011824s-gt-0-006524s" class="headerlink" title="最终版本（0.011824s -&gt; 0.006524s)"></a>最终版本（0.011824s -&gt; 0.006524s)</h3><p>这版其实不算我们的优化，主要是重构代码，矩阵乘向量部分转化为矩阵乘矩阵问题，调用 cuBLAS 处理，另一个优势是分两个 kernel 分析更容易定位问题</p>
<p>求平均的 kernel 可以发现已经与 roofline 斜线部分重合，虽然只达到 2% 的计算能力峰值。Memory Workload Analysis 一栏发现，整体利用了 80% 以上的 DRAM 带宽，达到 977.73 GB&#x2F;s，或者换一种计算方式，我们构造了 4G 的数据，使用 4.40ms（此 kernel 时间）读取加消费完，平均吞吐量是 909GB&#x2F;s，甚至超过 volta 架构白皮书中声称的 900GB&#x2F;sec 峰值带宽。</p>
<p>另矩阵乘部分观察到落在 roofline 房顶，达到 84% fp32 理论性能极限</p>
<p>以上这些观察都意味着我们的优化已经接近极限</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>TODO 关注哪些指标<br>TODO 补充 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/464172074">https://zhuanlan.zhihu.com/p/464172074</a></p>
<p>TODO compute peak perf 是怎么计算出来的 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/231302709">https://zhuanlan.zhihu.com/p/231302709</a></p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>本地 NSight Compute 和服务器上的 NSight Compute 版本一定要一致（包括小版本），否则某些指标不会显示</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%8A%80%E6%9C%AF/" class="category-chain-item">技术</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/CUDA/">#CUDA</a>
      
        <a href="/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/">#性能优化</a>
      
        <a href="/tags/Profiling/">#Profiling</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>使用 Nsight Compute 对算子进行优化</div>
      <div>https://vicety.github.io/2023/12/10/Analysis-Driven-Optimization/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>vicety</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年12月10日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/02/19/%E5%86%99%E7%B1%BB%E5%9E%8B%E5%AE%89%E5%85%A8%E7%9A%84Python/" title="Python with Type">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Python with Type</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/11/30/%E5%A6%82%E4%BD%95%E4%B8%BAcuda-kernel%E9%80%89%E6%8B%A9block-grid-size/" title="为 CUDA kernel 选择合适的 grid / block size">
                        <span class="hidden-mobile">为 CUDA kernel 选择合适的 grid / block size</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  <article id="comments" lazyload>
    
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.4.17/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"p7Skzx1UifgNosmr5ms2nILv-gzGzoHsz","appKey":"W73NSV1s3dRPtfceJDBsX5uP","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


  </article>


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="/js/leancloud.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
